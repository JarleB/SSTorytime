
# Knowledge and learning

You might think you know how to acquire knowledge. You learn stuff---how hard could it be?
You take a course, or read a book. But, of course, there is much more to it than that.

Knowing isn't just remembering. You might remember where the fire
extinguisher is, but when there's a fire, do you know what to do? You
might have seen people knit a sweater, but could you do it?  We need
to use knowledge to actually `know' it. It's like food. You can collect it,
but you don't know what it is until you actually taste it.

## Wikis and why documentation is so bad

There's an old joke: that Wikis are places where knowledge goes to
die. Well meaning individuals may invest hours of work to write
something down for others. But no one forms an intimate relationship
to what has been written, it is not knowledge. It's just a graveyard
of bits and bytes that means nothing to anyone except the person who
wrote it. The same is true of any book, even those written by a well
meaning author. Some books might be vanity projects, not meant to be
embraced, but teaching books always try to reach an audience in some
way. Success or failure depends on building a rapport with a certain reader.
You can't reach everyone, so you aim for a few.

## What is knowing, actually?

Knowledge is more than memory. You can `learn' a page of text by
heart and still have no idea how to use it. As long as it remains a
lump of data, in your head or simply on paper, in a computer, or and the back of your hand,
it's of little use to you. Knowledge comes from knowing things deeply---by
forming a relationship to material. You know something when you know it like a friend.
You won't have go and look up details because access will be integrated into your
conscious experience and awareness of environments you know about. This is what it
means to have knowledge at your fingertips. We are designed to use our hands
and fingers. 


Writing stuff down is useless if no one reads it. This is why Wikis,
knowledge bases, and expert systems often fail. Most Wikis are
intended as 1:N communication.  Wikipedia can succeed due to scale:
it's N:N for large N, which means the information is passed through a
human brain frequently.

https://medium.com/@mark-burgess-oslo-mb/the-failure-of-knowledge-management-5d97bb748fc3

This is one reason why current AI language models that seem to `know things'
are in fact as clueless about their subject matter as you are the day after reading
their results.

## Limits on learning

Even simply cramming facts by brute force memorization is hard. There
is a limit to the scaling of learning. Even machine learning can't
lead to endless improvements, because the cost-benefit of finding the
right data and automating learning rapidly becomes untenable. Adding
resources to capture every last variable takes too long and costs too
much energy. It's most effective for common short-term experiential
learning, like pattern recognition in the immediate environment of
sensory pre-processing. After that, a Monte Carlo approach to
guesswork or ``prediction'' would be much more energy efficient.

Of course, one might say that humans have developed vast learning
resources--far greater perhaps than could be accounted for by a strict
cost-benefit analysis.  Doesn't this indicate that learning still has
a benefit?  But evolution doesn't direct the course of change on an
actionable timescale, it merely expresses a lukewarm approval over
benefits still to be spat upon by competitive opportunism.  Adding new
data will at some point plateau in terms of semantics or meaningful
features.  What remains is quite battle tested over a wide range of
contexts but might be only very rarely useful.
What is rarely spoken about in machine learning is the importance of
forgetting data: of post selection to keep only the most useful
pattern memories.

We also need to expose contextual knowledge.  Brute force learning
also doesn't discriminate by context: there is a far greater number of
seemingly-infinite combinatoric contexts for relevant data. Each will
have its own learning plateau associated with its characteristic scales
(particularly timescales). In machine learning, transformer
architectures have discovered this, but they are still based mainly on
recall.

Learning examines spacelike and timelike invariants.

* Learning things discovers mesoscopic invariants snapshot structures.
* Learning stories discovers behavioural invariants (mesoscopic sequences).


## Context: where, when, and strategy. A Scene Description Language?

Consciousness is not the hard problem of knowledge: context is.

When we describe the context in which we learned something we are describing the key
for finding it later.

We use the term context in several ways.

* **Strategic context***: When taking notes, we use headers and headlines to descibe a context of "aboutness":
keywords, perhaps in a phrase, that describe what we'll find in the passage.
This is a strategioc use of context. It says: my strategy was to put this here so you would
find it, if you looked up these keywords.

* **Scene description**: thanks to our senses, the context in which we think of something
may depend on a complex web of happenings (of causality) that's ultimately summarized by a sort of 
snapshot of *the state the scene*
that we think of as context.
The fullest possible description of context is thus a background story for the moment.
Think of forensic investigators solving a mystery. They assemble context as factual descriptors,
causal motives and how all of the above come together. 

We might imagine a Scene Description Language
to be the ultimate goal of context. Our hypothesis here is that this imaginary Scene Description Language
is just a form of the semantic spacetime that we are developing under N4L.

In IT knowledge systems, the concept of a formal "ontology" has found popularity.
This is the idea that there is a kind of spanning tree of correct categorical meanings
for knowledge. Unfortunately, this idea has been shown to be flawed, if not merely false,
many times. There are always many possible spanning trees, or interpretations that classify
meaning. When we tell a story we might start at the beginning and follow chronology, or we might start
with the outcome and work backwards. We might hop back and forth atemporally in order
to discuss the relevance of the parts to the whole. The context is not a spanning tree.

By following trails of thought, we are assembling a trail of prerequisites that
our brains, evolved for navigation in a landscape, can understand. If we 
fall prey to the conceits of perfect logic we will tend to over-constrain information
so that it becomes impossible to find without the precise criteria used to store
it. This probably the most common mistake in using tools like RDF with OWL (the web ontology language)
as these as based on first order logic.

This ends up working against us, because we need to reproduce the precise lookup key
to find what we're looking for---and we might not even be clear about what we're
looking for! The main benefit of memory is creative composition of ideas, by "mixing it up".

Think of learning a language. You can quickly the vocabulary but you
can't recall it on demand. It takes several years of constant use to
develop the recall methods.


In this project, we look at trying to separate the processes that
manage different scales of learning to establish efficient recall structures
after post-selection.

## Completeness and scale

When parsing new information, word by word, we have no idea what a
complete sentence is going to mean or be about.  We have to wait for
the finish line to go back over the full expression.  We are
constantly buffering information in chunks to ascribe meaning to it.
The same applies to a paragraph. One sentence may say little, but a
full paragraph can make a point.  We can never be certain when a stream
of information is finished or complete in some sense. There can be diversions
to order, parentheses that form the grammar of the telling of the story, and so on.

Think about trying to learn a language. First we start by repetition
of words and phrases. We can use our basic cognitive skills to cram
words like Random Access Memory.  This leaves us only with parroting
skills. We don't end up with any knowledge of the grammar unless we go
back over many sentences and begin to search for patterns.  We have to
work analytically to learn more. We have to be active ourselves.  It's
not a job we can outsource to someone else. If you want to learn how
to make the tea and polish shoes, don't hire a butler. If you want to
understand how to arrange and organize learn history, don't delegate
to an assistant (especially and AI chat).

What we miss when trying to cram knowledge is a sense of `intent', i.e.
connecting what we're actually trying to do to what we can remember.
You miss what you actually want to say, and how to compose that
freely, without being constrained by a series of logical barriers or
brute for memory acts.
A single experience of being able to use a word in practice will burn itself
into memory more effectively than a hundred hours of cramming.

Some of the answer to this lies in grammar: the rules of composition, but
it's more than that. Ideation is the process of finding words that
you've never seen together before and placing that new combination
and its meaning into a new context.


The old jibe `those who cannot do teach' is about this mismatch
between crammed knowledge and applied knowledge. Of course, the best
teachers have already done things and try to pass on that experience of
transmuting facts into action, though students won't necessarily have
the context or experience to follow them. A good teacher creates that
context. This is why we are asked to do exercises, not just reading.

As a student, my own first step in processing the new information
was to rewrite it in my own words; create my own story from the one
given to me. How would I explain it to someone else? I can't just be actor
memorizing lines, I need to have tried and failed.

So how can *assist* humans to arrive at that understanding, especially if they
are challenged in memory ability or have cognitive difficulties?
They need to recall a basic vocabulary of things and activities (nouns and verbs).
When they are missing a word, how can we prompt something appropriate?

The bottom line is: knowledge is like knowing a friend. It's the same brain,
and the same skill we use for both. You start by tipping your hat when you
recognize someone, then you might say good morning. You'll stop for a chat,
maybe have a coffee and have your first date, but little of this
will stay with you unless you start to care. Only when you long for their
return can you cay that you know something about them.

## Phases

* You take notes, in little patches of things to remember, hoping to trigger a larger memory.

* When you have enough notes, you start to put them in some kind of order, so that things that belong
together can be found together. Neurons that fire together, wire together! 

* We find out quickly that there can't be a box for everything. If we try to be too precise, we'd
package everything in its own box, because every situation is unique. Instead, we realize that
knowledge is about we approximate ideas by looking for what's similar rather than what's different.
There's a time for collation and a time for discrimination. There's a Chinese proverb that things group into
categories, but people divide into groups. It suggests that people are contentious, while everything else
can be treated with a more welcoming approach. Knowledge is also about being open minded!

* Writing down errors and misunderstandings, and resolving them helps us to overcome barriers
to seeing patterns. Some of the best knowledge comes from overcoming a misunderstand---from asking the right question.

* It takes work (revising and rewriting, re-ordering, tidying, copying and eliminating, etc)
to become familiar with materials. It's the start of a relationship, to making a new friend.

* Rhymes, slogans, and catch phrases are useful for remembering ideas, because they draw on
our affinity for pattern and motor skills.

## The goal

We already have the trusty index to help us with random recall. An index offers a way into a body of
tidy knowledge. How then, could a more sophisticated data structure help us to find what we need?
In the past people have tried before with taxonomies, Topic Maps, and Resource Description Frameworks based on 
technical Ontologies. These are all useful, but only if users find their own methods for using them.
It's possible to abuse them because they don't guide users towards good habits: have no constraints.

Bureaucracy is an early attempt to manage knowledge. 
The mistake bureaucrats make is the same one logicians make: they force fruitless work onto the user
at the wrong moment. Forcing users to work too much at the beginning to make everything fit into a perfect box
of someone else's making acts as a disincentive rather than an encouragement. So we want to take away the barriers
of documentation, letting users write things down in scraps and notes as they like. Then we want to
incentivize them to come back and make that knowledge a work of beauty in their own minds---something they
can fall in love with.


## Alphabets and classifications

Alphabets are collections of standard symbols. The Greek alphabet (from the first
two symbols A,B) were a turning point for script.
What an alphabet does is to offer up a smallish `menu' of items for
reconstructing intentional words. The words have meanings, and we
remember the sounds well.  

Words are associated with sounds. Seeing the sounds (unless you're
deaf) is possible with a phonetic encoding, and this is what an
alphabet enables with only a small number of symbols. In symbolic
languages, like Chinese and ancient Egyptian, symbolic were tied
visually rather than phonetically to meanings, i.e. independently of the sounds.
Later, the
need for phonetics was overcome by using phonetic transcriptions based
on a small number of well known symbols. They had to invent a
surrogate alphabet for names and foreign words, in particular.



## Classification and misclassification

As we write down our notes, we can't possibly ensure that every item
is correctly classified into the right chapter, or in the right
order. We still need to be able to find that misplaced book in the
library, or a missing person amongst the busy goings-on of the
world. It's an unavoidable issue, but it can be remedied by regular
tidying, rehearsing procedures, maintenance of materials, and so on.
It can be remedied, but never solved once and for all. As soon as we
stop, the knowledge dies. What we realize, then, is that the need
for `searching' cannot be eliminated as long as knowledge is alive.
Indexing is the tool that makes that possible, alongside the storyline
of tables of contents and order.

Over-constraint, or over-doing classification leads to ideas like data normalization,
or the normal forms that were invented for databases when humans were supposed
to type in the data by hand. Consistency errors were an issue, so a scheme of normalized
forms was invented to reduce the possibility of error.

If we over-constrain data by placing them perfectly so that they can only be found with
the precise (highly precise) key, then we reduce the chances of finding the information
to only perfectly asked questions. That's unhelpful. The goal is to make things easy to find,
and to encourage browsing and stumbling across possibly relevant connections.

