
# Knowledge and learning

You might think you know how to acquire knowledge. You learn stuff---how hard could it be?
You take a course, or read a book. But, of course, there is much more to it than that.

Knowing isn't just remembering. You might remember where the fire
extinguisher is, but when there's a fire, do you know what to do? You
might have seen people knit a sweater, but could you do it?  We need
to use knowledge to actually `know' it. It's like food. You can collect it,
but you don't know what it is until you actually taste it.

## Wikis and why documentation is so bad

There's an old joke: that Wikis are places where knowledge goes to
die. Well meaning individuals may invest hours of work to write
something down for others. But no one forms an intimate relationship
to what has been written, it is not knowledge. It's just a graveyard
of bits and bytes that means nothing to anyone except the person who
wrote it. The same is true of any book, even those written by a well
meaning author. Some books might be vanity projects, not meant to be
embraced, but teaching books always try to reach an audience in some
way. Success or failure depends on building a rapport with a certain reader.
You can't reach everyone, so you aim for a few.

## What is knowing, actually?

Knowledge is more than memory. You can `learn' a page of text by
heart and still have no idea how to use it. As long as it remains a
lump of data, in your head or simply on paper, in a computer, or and the back of your hand,
it's of little use to you. Knowledge comes from knowing things deeply---by
forming a relationship to material. You know something when you know it like a friend.
You won't have go and look up details because access will be integrated into your
conscious experience and awareness of environments you know about. This is what it
means to have knowledge at your fingertips. We are designed to use our hands
and fingers. 


Writing stuff down is useless if no one reads it. This is why Wikis,
knowledge bases, and expert systems often fail. Most Wikis are
intended as 1:N communication.  Wikipedia can succeed due to scale:
it's N:N for large N, which means the information is passed through a
human brain frequently.

https://medium.com/@mark-burgess-oslo-mb/the-failure-of-knowledge-management-5d97bb748fc3

This is one reason why current AI language models that seem to `know things'
are in fact as clueless about their subject matter as you are the day after reading
their results.

## Limits on learning

Even simply cramming facts by brute force memorization is hard. There
is a limit to the scaling of learning. Even machine learning can't
lead to endless improvements, because the cost-benefit of finding the
right data and automating learning rapidly becomes untenable. Adding
resources to capture every last variable takes too long and costs too
much energy. It's most effective for common short-term experiential
learning, like pattern recognition in the immediate environment of
sensory pre-processing. After that, a Monte Carlo approach to
guesswork or ``prediction'' would be much more energy efficient.

Of course, one might say that humans have developed vast learning
resources--far greater perhaps than could be accounted for by a strict
cost-benefit analysis.  Doesn't this indicate that learning still has
a benefit?  But evolution doesn't direct the course of change on an
actionable timescale, it merely expresses a lukewarm approval over
benefits still to be spat upon by competitive opportunism.  Adding new
data will at some point plateau in terms of semantics or meaningful
features.  What remains is quite battle tested over a wide range of
contexts but might be only very rarely useful.
What is rarely spoken about in machine learning is the importance of
forgetting data: of post selection to keep only the most useful
pattern memories.

We also need to expose contextual knowledge.  Brute force learning
also doesn't discriminate by context: there is a far greater number of
seemingly-infinite combinatoric contexts for relevant data. Each will
have its own learning plateau associated with its characteristic scales
(particularly timescales). In machine learning, transformer
architectures have discovered this, but they are still based mainly on
recall.

Learning examines spacelike and timelike invariants.

* Learning things discovers mesoscopic invariants snapshot structures.
* Learning stories discovers behavioural invariants (mesoscopic sequences).


## Context: where, when

When we describe the context in which we learned something we are describing the key
for finding it later.

By following trails of thought, we are assembling a trail of prerequisites that
our brains, evolved for navigation in a landscape, can understand. If we 
fall prey to the conceits of perfect logic we will tend to over-constrain information
so that it becomes impossible to find without the precise criteria used to store
it. This probably the most common mistake in using tools like RDF with OWL (the web ontology language)
as these as based on first order logic.

This ends up working against us, because we need to reproduce the precise lookup key
to find what we're looking for---and we might not even be clear about what we're
looking for! The main benefit of memory is creative composition of ideas, by "mixing it up".

Think of learning a language. You can quickly the vocabulary but you
can't recall it on demand. It takes several years of constant use to
develop the recall methods.


In this project, we look at trying to separate the processes that
manage different scales of learning to establish efficient recall structures
after post-selection.

## Completeness and scale

When parsing new information, word by word, we have no idea what a
complete sentence is going to mean or be about.  We have to wait for
the finish line to go back over the full expression.  We are
constantly buffering information in chunks to ascribe meaning to it.
The same applies to a paragraph. One sentence may say little, but a
full paragraph can make a point.  We can never be certain when a stream
of information is finished or complete in some sense. There can be diversions
to order, parentheses that form the grammar of the telling of the story, and so on.

Think about trying to learn a language. First we start by repetition
of words and phrases. We can use our basic cognitive skills to cram
words like Random Access Memory.  This leaves us only with parroting
skills. We don't end up with any knowledge of the grammar unless we go
back over many sentences and begin to search for patterns.  We have to
work analytically to learn more. We have to be active ourselves.  It's
not a job we can outsource to someone else. If you want to learn how
to make the tea and polish shoes, don't hire a butler. If you want to
understand how to arrange and organize learn history, don't delegate
to an assistant (especially and AI chat).

What we miss when trying to cram knowledge is a sense of `intent', i.e.
connecting what we're actually trying to do to what we can remember.
You miss what you actually want to say, and how to compose that
freely, without being constrained by a series of logical barriers or
brute for memory acts.
A single experience of being able to use a word in practice will burn itself
into memory more effectively than a hundred hours of cramming.

Some of the answer to this lies in grammar: the rules of composition, but
it's more than that. Ideation is the process of finding words that
you've never seen together before and placing that new combination
and its meaning into a new context.


The old jibe `those who cannot do teach' is about this mismatch
between crammed knowledge and applied knowledge. Of course, the best
teachers have already done things and try to pass on that experience of
transmuting facts into action, though students won't necessarily have
the context or experience to follow them. A good teacher creates that
context. This is why we are asked to do exercises, not just reading.

As a student, my own first step in processing the new information
was to rewrite it in my own words; create my own story from the one
given to me. How would I explain it to someone else? I can't just be actor
memorizing lines, I need to have tried and failed.

So how can *assist* humans to arrive at that understanding, especially if they
are challenged in memory ability or have cognitive difficulties?
They need to recall a basic vocabulary of things and activities (nouns and verbs).
When they are missing a word, how can we prompt something appropriate?

The bottom line is: knowledge is like knowing a friend. It's the same brain,
and the same skill we use for both. You start by tipping your hat when you
recognize someone, then you might say good morning. You'll stop for a chat,
maybe have a coffee and have your first date, but little of this
will stay with you unless you start to care. Only when you long for their
return can you cay that you know something about them.

## Phases

* You take notes, in little patches of things to remember, hoping to trigger a larger memory.

* When you have enough notes, you start to put them in some kind of order, so that things that belong
together can be found together. Neurons that fire together, wire together! 

* We find out quickly that there can't be a box for everything. If we try to be too precise, we'd
package everything in its own box, because every situation is unique. Instead, we realize that
knowledge is about we approximate ideas by looking for what's similar rather than what's different.
There's a time for collation and a time for discrimination. There's a Chinese proverb that things group into
categories, but people divide into groups. It suggests that people are contentious, while everything else
can be treated with a more welcoming approach. Knowledge is also about being open minded!

* Writing down errors and misunderstandings, and resolving them helps us to overcome barriers
to seeing patterns. Some of the best knowledge comes from overcoming a misunderstand---from asking the right question.

* It takes work (revising and rewriting, re-ordering, tidying, copying and eliminating, etc)
to become familiar with materials. It's the start of a relationship, to making a new friend.

## The goal

We already have the trusty index to help us with random recall. An index offers a way into a body of
tidy knowledge. How then, could a more sophisticated data structure help us to find what we need?
In the past people have tried before with taxonomies, Topic Maps, and Resource Description Frameworks based on 
technical Ontologies. These are all useful, but only if users find their own methods for using them.
It's possible to abuse them because they don't guide users towards good habits: have no constraints.

Bureaucracy is an early attempt to manage knowledge. 
The mistake bureaucrats make is the same one logicians make: they force fruitless work onto the user
at the wrong moment. Forcing users to work too much at the beginning to make everything fit into a perfect box
of someone else's making acts as a disincentive rather than an encouragement. So we want to take away the barriers
of documentation, letting users write things down in scraps and notes as they like. Then we want to
incentivize them to come back and make that knowledge a work of beauty in their own minds---something they
can fall in love with.


## Alphabets and classifications

Alphabets are collections of standard symbols. The Greek alphabet (from the first
two symbols A,B) were a turning point for script.
What an alphabet does is to offer up a smallish `menu' of items for
reconstructing intentional words. The words have meanings, and we
remember the sounds well.  

Words are associated with sounds. Seeing the sounds (unless you're
deaf) is possible with a phonetic encoding, and this is what an
alphabet enables with only a small number of symbols. In symbolic
languages, like Chinese and ancient Egyptian, symbolic were tied
visually rather than phonetically to meanings, i.e. independently of the sounds.
Later, the
need for phonetics was overcome by using phonetic transcriptions based
on a small number of well known symbols. They had to invent a
surrogate alphabet for names and foreign words, in particular.



## Classification and misclassification

As we write down our notes, we can't possibly ensure that every item
is correctly classified into the right chapter, or in the right
order. We still need to be able to find that misplaced book in the
library, or a missing person amongst the busy goings-on of the
world. It's an unavoidable issue, but it can be remedied by regular
tidying, rehearsing procedures, maintenance of materials, and so on.
It can be remedied, but never solved once and for all. As soon as we
stop, the knowledge dies. What we realize, then, is that the need
for `searching' cannot be eliminated as long as knowledge is alive.
Indexing is the tool that makes that possible, alongside the storyline
of tables of contents and order.

Over-constraint, or over-doing classification leads to ideas like data normalization,
or the normal forms that were invented for databases when humans were supposed
to type in the data by hand. Consistency errors were an issue, so a scheme of normalized
forms was invented to reduce the possibility of error.

If we over-constrain data by placing them perfectly so that they can only be found with
the precise (highly precise) key, then we reduce the chances of finding the information
to only perfectly asked questions. That's unhelpful. The goal is to make things easy to find,
and to encourage browsing and stumbling across possibly relevant connections.


## SSTorytelling

To devise a knowledge management system, our aims are:

* To assist in overcoming human limitations, while respecting the reason for them.
* To devise a way of getting experiences and thoughts into a computer representation
that will be used actively and immediately. For this, we shall devise a language N4L
or Notes For Learning.

The structure of cognition lies in decoding input into a lasting invariant
representation, which is linked to a rich set of very familiar contexts,
 with reactive output and post-associative feedback that expands the integration of what you've
memorized.

The distinction between input representation and recall or information
``usability'' (the process of distilling something useful) is the
basic problem. It's the same process we face when attempting to learn
a foreign language: out mental model is spontaneous, but forcing it
through the bottleneck of language is hard, because we don't know all
the words and we may never have tried to say what we want to say
before.  We would like to ensure that there is a sufficient reservoir
of examples to draw on and use as templates.

We may try to `cram' words into memory by repetition, but in the heat of the moment
we're unable to recall any of it. That's because memory lookup is contextual,
and the context in which we experience it and the context in which we try to learn it
are totally different---and we don't know how to connect the two.

Part of our difficulty in making notes and organizing is also our lack
of an overview. It doesn't take very much information to fill a page
and exceed our visual resolution. Even if we could organize everything
on a single sheet, our ability to resolve and comprehend it is
limited. So what we need is a way of integrating disjointed fragments
of note-taking to create that integrated model.

## Tidying up is a learning strategy

If you do it once, it just hides information. If you keep doing to improve and fiddle with the
organization of things, you become intimately and cognitively familiar with the placement and
usefulness of things. This is what our brains do. There has to be re-use activity, involving motor
functions to extract things from their tidy places.

## Memory strategy and organization

When we tidy, often we create subject boxes first. It's by putting something in the right box that
we believe we understand it.
For example, when writing this, I write a number of section titles and try to collect
everything related to it under each heading..

Reorganizing notes post-hoc is very time consuming, because it requires multiple passes and many decisions.
Unless we intentionally place fragments of experience into boxes initially and intentionally,
we lose the economic benefits of experience. Intent thus serves an economic function for a cognitive
agent.

But there is a problem with this. When we group things together, we made trade-offs and approximations
that we might not agree with later. For instance, is a a duckbilled platypus a mammal or a bird.
It's a warm blooded species that lays eggs. It flouts the boundaries of the largest boxes in biology
by belonging to two boxes or a box all by itself. When we rely on box logic, things always go wrong in the end.
This is the trouble with ontologies as we use them in technology today. The main benefit of doing this
is that it forces us to revisit the model over and over again and make connections. We then identify knowledge
with those "aha!" moments at which we independently had insights. Those are the moments we remember
and can recall what we learned.

Our philosophical affectations have led us to go too far, however, when we form entire world *ontologies*
and try to fit everything into a single common spanning tree of knowledge. This is an abuse of process,
because ontologies are only outgrowths of coordinate systems for indexing {\em contexts}. They belong
to a separate sensory language that represents experience, not the concepts derived from cumulative processing and metaphoric expansion.

The temptation to continue classifying and sub-classifying by inventing ad hoc discriminators is a 
powerful tendency that occasionally runs riot and appeals to the bureaucratic mind. Even though applying
discriminators from general experience ad hoc seems like a smart approach, it easily
 leads to tunnel vision.




## N4L, a note taking language

### Minimal syntax

We would like a simple free text format for entering data, without
specialized encodings, for the first phase of jotting down items of
information that we want to learn and know.  This language must support Unicode for multi-language
support.  Memory data may come in a variety of media formats: text,
images, audio etc. When we are searching, however, symbolic language
is a convenient interaction format. So, in a first instance, we can
use pattern matching transducers to convert multimedia formats into
``alt texts'' and categorize those in a knowledge reasoning structure.
This may not strictly be necessary in the long run, but it's a useful
place to begin: in particular we need the ability to identify
sub-parts of an image of audio segment in order to cross-reference it.

The basic syntactic format of the contextual note taking language takes the following form:
